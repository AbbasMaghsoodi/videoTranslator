# -*- coding: utf-8 -*-
"""videoTranslatorByAudio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JeEO1_uv8mPN6rv9FRVPi3OaM9Xf23Cj
"""

import subprocess
import ffmpeg
import moviepy.editor as mpe
import rubberband
import googletrans
import gtts
import pyttsx3
import mediapipe

def translate_video(video_file, target_language, output_file):
  """Translates the voice in a video to a target language and generates a lip-synced audio file of the translation.

  Args:
    video_file: The path to the input video file.
    target_language: The target language code.
    output_file: The path to the output audio file.
  """

  # Extract the audio from the video file.
  audio_file = ffmpeg.input(video_file).output("audio.wav", format="wav").run(capture_output=True)[1]

  # Translate the audio file to the target language.
  translator = googletrans.Translator()
  translated_text = translator.translate(audio_file.decode("utf-8"), dest=target_language).text


  # Generate a lip-synced audio file of the translation.
  engine = pyttsx3.Engine()
  engine.setProperty("rate", 150)
  tts = gtts.gTTS(text=translated_text, lang=target_language, voice=engine.getProperty("voice"))
  tts.save(output_file)

  # Warp the translated audio to match the length and tone of the original audio.
  rubberband = rubberband.RubberBand()
  rubberband.set_rate(0.5)
  warped_audio = rubberband.process(output_file)

  # Write the warped audio to a new file.
  with open("warped_audio.wav", "wb") as f:
    f.write(warped_audio)

def replace_audio_in_video(video_file, audio_file, output_file):
  """Replaces the audio in a video with a new audio file.

  Args:
    video_file: The path to the input video file.
    audio_file: The path to the input audio file.
    output_file: The path to the output video file.
  """

  ffmpeg.input(video_file).input(audio_file, ss=0.5).output(output_file, pix_fmt="yuv420p", acodec="aac").run()

if __name__ == "__main__":
  # Get the video file from the user.
  video_file = input("Enter the path to the video file: ")

  # Get the target language from the user.
  target_language = input("Enter the target language: ")

  # Translate the video and generate a lip-synced audio file of the translation.
  translate_video(video_file, target_language, "output.wav")

  # Replace the audio in the video with the translated audio.
  replace_audio_in_video(video_file, "warped_audio.wav", "output_video.mp4")

  # Print a message to the user.
  print("The output video has been generated and saved as `output_video.mp4`.")

import mediapipe as mp
import rubberband
import moviepy.editor as mpe
import googletrans
import gtts
import pyttsx3

def translate_video(video_file, target_language, output_file):
  """Translates the voice in a video to a target language and generates a lip-synced audio file of the translation.

  Args:
    video_file: The path to the input video file.
    target_language: The target language code.
    output_file: The path to the output audio file.
  """

  # Extract the audio from the video file.
  video = mpe.VideoFileClip(video_file)
  audio = video.audio

  # Translate the audio file to the target language.
  translator = googletrans.Translator()
  translated_text = translator.translate(audio.read(), dest=target_language).text

  # Generate a lip-synced audio file of the translation.
  engine = pyttsx3.Engine()
  engine.setProperty("rate", 150)
  tts = gTTS(text=translated_text, lang=target_language, voice=engine.getProperty("voice"))
  tts.save("tmp.wav")

  # Create a temporary audio file to save the lip-synced audio to.
  tmp_audio_clip = mpe.AudioFileClip("tmp.wav")

  # Extract the pitch of the original voice in the video.
  with mp.solutions.holistic.Holistic() as holistic:
    for results in holistic.process(video.read_frame()):
      pitch = results.face_landmarks.landmark[mp.solutions.holistic.HolisticLandmark.LEFT_EAR].z * 440

  # Shift the pitch of the translated audio to match the pitch of the original audio.
  shifted_audio_clip = rubberband.RubberBand()
  shifted_audio_clip.set_rate(original_pitch / target_pitch)
  shifted_audio_clip = shifted_audio_clip.process(tmp_audio_clip.read())

  # Replace the audio in the video with the cloned audio.
  video.audio = mpe.AudioSegment(shifted_audio_clip)

  # Write the output video file.
  video.write_videofile(output_file, audiocodec="aac")

  # Delete the temporary audio file.
  tmp_audio_clip.close()
  os.remove("tmp.wav")


if __name__ == "__main__":
  # Get the video file from the user.
  video_file = input("Enter the path to the video file: ")

  # Get the target language from the user.
  target_language = input("Enter the target language: ")

  # Translate the video and generate a lip-synced audio file of the translation.
  translate_video(video_file, target_language, "output.mp4")

  # Print a message to the user.
  print("The output video has been generated and saved as `output.mp4`.")

import mediapipe as mp
import rubberband
import moviepy.editor as mpe
import googletrans
import gtts
import pyttsx3

def transcribe_audio(audio_file_path, language_code="en-US"):
  """Transcribes an audio file to text.

  Args:
    audio_file_path: The path to the audio file.
    language_code: The language code of the audio file.

  Returns:
    A string containing the text transcription of the audio file.
  """

  client = speech_v1.SpeechClient()

  with io.open(audio_file_path, "rb") as audio_file:
    content = audio_file.read()

  audio = speech_v1.types.Audio(content=content)
  config = speech_v1.types.RecognitionConfig(
      language_code=language_code,
      encoding="LINEAR16",
      sample_rate_hertz=16000
  )

  response = client.recognize(config=config, audio=audio)

  if response.results:
    return response.results[0].alternatives[0].transcript
  else:
    return None


def translate_video(video_file, target_language, output_file):
  """Translates the voice in a video to a target language and generates a lip-synced audio file of the translation.

  Args:
    video_file: The path to the input video file.
    target_language: The target language code.
    output_file: The path to the output audio file.
  """

  # Extract the audio from the video file.
  video = mpe.VideoFileClip(video_file)
  audio = video.audio

  # Transcribe the audio to text.
  transcribed_text = transcribe_audio(audio.read())

  # Translate the text to the target language.
  translator = googletrans.Translator()
  translated_text = translator.translate(transcribed_text, dest=target_language).text

  # Generate a lip-synced audio file of the translation.
  engine = pyttsx3.Engine()
  engine.setProperty("rate", 150)
  tts = gTTS(text=translated_text, lang=target_language, voice=engine.getProperty("voice"))
  tts.save("tmp.wav")

  # Create a temporary audio file to save the lip-synced audio to.
  tmp_audio_clip = mpe.AudioFileClip("tmp.wav")

  # Extract the pitch of the original voice in the video.
  with mp.solutions.holistic.Holistic() as holistic:
    for results in holistic.process(video.read_frame()):
      pitch = results.face_landmarks.landmark[mp.solutions.holistic.HolisticLandmark.LEFT_EAR].z * 440

  # Shift the pitch of the translated audio to match the pitch of the original audio.
  shifted_audio_clip = rubberband.RubberBand()
  shifted_audio_clip.set_rate(original_pitch / target_pitch)
  shifted_audio_clip = shifted_audio_clip.process(tmp_audio_clip.read())

  # Replace the audio in the video with the cloned audio.
  video.audio = mpe.AudioSegment(shifted_audio_clip)

  # Write the output video file.
  video.write_videofile(output_file, audiocodec="aac")

  # Delete the temporary audio file.
  tmp_audio_clip.close()
  os.remove("tmp.wav")


if __name__ == "__main__":
  # Get the video file from the user.
  video_file = input("Enter the path to the video file: ")

  # Get the target language from the user.
  target_language = input("Enter the target language: ")

  # Translate the video and generate a lip-synced audio file of the translation.
  translate_video(video_file, target_language, "output.mp4")

  # Print a message to the user.
  print("The output video has been generated and saved as `output.mp4`.")

pip install moviepy --upgrade

import mediapipe as mp
import rubberband
import moviepy.editor as mpe
import googletrans
import gTTS
import pyttsx3

def translate_video(video_file, target_language, output_file):
  """Translates the voice in a video to a target language and generates a lip-synced audio file of the translation.

  Args:
    video_file: The path to the input video file.
    target_language: The target language code.
    output_file: The path to the output audio file.
  """

  # Extract the audio from the video file.
  video = mpe.VideoFileClip(video_file)
  audio = video.audio

  # Translate the audio file to the target language.
  translator = googletrans.Translator()
  translated_text = translator.translate(audio.data, dest=target_language).text

  # Generate a lip-synced audio file of the translation.
  engine = pyttsx3.Engine()
  engine.setProperty("rate", 150)
  tts = gTTS(text=translated_text, lang=target_language, voice=engine.getProperty("voice"))
  tts.save("tmp.wav")

  # Create a temporary audio file to save the lip-synced audio to.
  tmp_audio_clip = mpe.AudioFileClip("tmp.wav")

  # Extract the pitch of the original voice in the video.
  with mp.solutions.holistic.Holistic() as holistic:
    for results in holistic.process(video.read_frame()):
      pitch = results.face_landmarks.landmark[mp.solutions.holistic.HolisticLandmark.LEFT_EAR].z * 440

  # Shift the pitch of the translated audio to match the pitch of the original audio.
  shifted_audio_clip = rubberband.RubberBand()
  shifted_audio_clip.set_rate(original_pitch / target_pitch)
  shifted_audio_clip = shifted_audio_clip.process("tmp.wav")

  # Replace the audio in the video with the cloned audio.
  video.audio = shifted_audio_clip

  # Write the output video file.
  video.write_videofile(output_file, audiocodec="aac")

  # Delete the temporary audio file.
  tmp_audio_clip.close()
  os.remove("tmp.wav")


if __name__ == "__main__":
  # Get the video file from the user.
  video_file = input("Enter the path to the video file: ")

  # Get the target language from the user.
  target_language = input("Enter the target language: ")

  # Translate the video and generate a lip-synced audio file of the translation.
  translate_video(video_file, target_language, "output.mp4")

  # Print a message to the user.
  print("The output video has been generated and saved as `output.mp4`.")

pip install mediapipe

pip install ffmpeg-python

pip install pyttsx3

pip install gtts

pip install googletrans==4.0.0rc1

!apt-get install -y build-essential cmake ninja-build pkg-config

pip install moviepy

!apt-get update

!apt-get install -y libsndfile-dev librubberband-dev

pip install rubberband

pip install gdown

!gdown https://github.com/RubberBand/rubberband/archive/refs/heads/main.zip

!unzip main.zip

!unzip /content/drive/MyDrive/input/rubberband-1.0.2.zip

cd rubberband-1.0.2

pip install setup.py bdist_wheel --universal

pip install .